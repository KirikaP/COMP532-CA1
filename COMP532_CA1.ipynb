{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements:\n",
    "\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The required class\n",
    "\n",
    "1. BanditMachine: A class that represents the bandit machine.\n",
    "2. EpsilonGreedy: A class that represents the epsilon greedy algorithm. \n",
    "3. UCB: A class that represents the upper confidence bound algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BanditMachine:\n",
    "\n",
    "class BanditMachine:\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms  # number of arms\n",
    "        \n",
    "        self.sigma = 1  # reward distribution's standard deviation\n",
    "        self.miu_list = np.random.normal(0, self.sigma, n_arms)  # initialize the mean of each arm's reward\n",
    "        self.counts = np.zeros(n_arms, dtype=int)  # record the number of each arm being pulled\n",
    "        self.values = np.zeros(n_arms)  # record the estimated value of each arm\n",
    "        \n",
    "    def update(self, arm, reward):\n",
    "        \"\"\"\n",
    "        update the arm's information\n",
    "        :param arm: int, index of the arm\n",
    "        :param reward: float, reward\n",
    "        \"\"\"\n",
    "        self.counts[arm] += 1\n",
    "        n = self.counts[arm]\n",
    "        value = self.values[arm]\n",
    "        self.values[arm] = ((n - 1) / n) * value + (1 / n) * reward # incremental update of the estimated value\n",
    "\n",
    "    def play(self, arm):\n",
    "        \"\"\"\n",
    "        play the arm and get the reward\n",
    "        :param arm: int, index of the arm\n",
    "        :return: float, reward\n",
    "        \"\"\"\n",
    "        if arm < 0 or arm >= self.n_arms:\n",
    "            raise ValueError(\"Arm index is out of range.\")\n",
    "        \n",
    "        # generate the reward\n",
    "        reward = np.random.normal(self.miu_list[arm], self.sigma)\n",
    "        # update the arm's information\n",
    "        self.update(arm, reward)\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EpsilonGreedy:\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, machines, epsilon):\n",
    "        \"\"\"\n",
    "        :param machines: BanditMachine, multi-armed bandit machine object\n",
    "        :param epsilon: float, probability of randomly selecting an arm\n",
    "        \"\"\"\n",
    "        self.machines = machines\n",
    "        self.epsilon = epsilon\n",
    "        self.arms = machines.miu_list\n",
    "        self.n_arms = len(self.arms)\n",
    "        self.counts = np.zeros(self.n_arms, dtype=int)  # record the number of each arm being pulled\n",
    "        self.values = np.zeros(self.n_arms, dtype=float)  # record the estimated value of each arm\n",
    "        self.total_counts = 0  # record the total number of arms being pulled\n",
    "\n",
    "    def select_arm(self):\n",
    "        # if there are arms that have never been pulled, then pull them\n",
    "        for arm in range(self.n_arms):\n",
    "            if self.counts[arm] == 0:\n",
    "                return arm\n",
    "        \n",
    "        # with probability epsilon, randomly select an arm, otherwise select the arm with the highest estimated value\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.n_arms)\n",
    "        else:\n",
    "            return np.argmax(self.values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        # update the estimated value of the chosen arm\n",
    "        self.total_counts += 1\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "    def play(self):\n",
    "        # choose an arm and pull it\n",
    "        chosen_arm = self.select_arm()\n",
    "        reward = self.machines.play(chosen_arm)\n",
    "        self.update(chosen_arm, reward)\n",
    "        return chosen_arm, reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB:\n",
    "    def __init__(self, machines, c):\n",
    "        \"\"\"\n",
    "        :param arms: list, each element is the mean of the reward distribution of an arm\n",
    "        :param c: float, a constant that controls the degree of exploration\n",
    "        \"\"\"\n",
    "        self.machines = machines\n",
    "        self.c = c \n",
    "        self.arms = machines.miu_list # the list of arms\n",
    "        self.n_arms = len(self.arms) # number of arms\n",
    "        self.counts = np.zeros(self.n_arms, dtype=int)  # record the number of each arm being pulled\n",
    "        self.values = np.zeros(self.n_arms, dtype=float)  # record the estimated value of each arm\n",
    "        self.total_counts = 0 # record the total number of arms being pulled\n",
    "\n",
    "    def select_arm(self):\n",
    "        # if there are arms that have never been pulled, then pull them\n",
    "        for i in range(self.n_arms):\n",
    "            if self.counts[i] == 0:\n",
    "                return i\n",
    "        \n",
    "        \n",
    "        # return the arm with the highest UCB value\n",
    "        ucb_values = self.values + self.c * np.sqrt(np.log(self.total_counts) / (self.counts + 1))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update(self, chosen_arm, reward):\n",
    "        # update the estimated value of the chosen arm\n",
    "        self.total_counts += 1\n",
    "        self.counts[chosen_arm] += 1\n",
    "        n = self.counts[chosen_arm]\n",
    "        value = self.values[chosen_arm]\n",
    "        self.values[chosen_arm] = ((n - 1) / n) * value + (1 / n) * reward\n",
    "\n",
    "    def play(self):\n",
    "        # choose an arm and pull it\n",
    "        chosen_arm = self.select_arm()\n",
    "        reward = self.machines.play(chosen_arm)\n",
    "        self.update(chosen_arm, reward)\n",
    "        return chosen_arm, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize BanditMachine with 10 arms\n",
    "bandit = BanditMachine(10)\n",
    "\n",
    "eps_01 = EpsilonGreedy(bandit,0.1)\n",
    "eps_00 = EpsilonGreedy(bandit,0)\n",
    "eps_001 = EpsilonGreedy(bandit,0.01)\n",
    "ucb_2 = UCB(bandit,2)\n",
    "ucb_1 = UCB(bandit,1)\n",
    "ucb_05 = UCB(bandit,0.5)\n",
    "optimal = np.argmax(bandit.miu_list)\n",
    "step = 1000\n",
    "runs = 2000\n",
    "\n",
    "# define the function to test the algorithm\n",
    "def test_algo(algo,runs,step):\n",
    "    Q_runs_lst = [[] for m in range(runs)]\n",
    "    Q_runs_lst_average = [0 for n in range(step)]\n",
    "    # 2d list, store the best action judgement variable of all runs of all steps, 1 is the correct action, 0 is not the best action\n",
    "    action_runs_lst = [[] for k in range(runs)]  \n",
    "\n",
    "    action_runs_lst_average = [0 for j in range(step)]  # record the average best action rate of each step by all runs\n",
    "    for run_times in range(0, runs):\n",
    "        k=1\n",
    "        algo.counts = np.zeros(algo.n_arms, dtype=int)\n",
    "        algo.values = np.zeros(algo.n_arms, dtype=float)\n",
    "        algo.total_counts = 0\n",
    "        while k<=step:\n",
    "            reward=algo.play()[1]\n",
    "            action=algo.play()[0]\n",
    "            Q_runs_lst[run_times].append(reward)\n",
    "            sum_best_action = 0\n",
    "            if action==optimal:\n",
    "                sum_best_action +=1\n",
    "            action_runs_lst[run_times].append(sum_best_action)\n",
    "            k += 1\n",
    "    for i in range(0, step):\n",
    "        for j in range(0, runs):\n",
    "            Q_runs_lst_average[i] = Q_runs_lst_average[i] + Q_runs_lst[j][i]\n",
    "            action_runs_lst_average[i] = action_runs_lst_average[i] + action_runs_lst[j][i]\n",
    "        Q_runs_lst_average[i] = Q_runs_lst_average[i] / runs\n",
    "        action_runs_lst_average[i] = action_runs_lst_average[i] / runs\n",
    "    return Q_runs_lst_average,action_runs_lst_average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the algorithm and get the average reward and best action rate of each step\n",
    "eps_01_reward,eps_01_rate = test_algo(eps_01,runs,step)\n",
    "eps_00_reward,eps_00_rate = test_algo(eps_00,runs,step)\n",
    "eps_001_reward,eps_001_rate = test_algo(eps_001,runs,step)\n",
    "ucb_1_reward,ucb_1_rate = test_algo(ucb_1,runs,step)\n",
    "ucb_05_reward,ucb_05_rate = test_algo(ucb_05,runs,step)\n",
    "ucb_2_reward,ucb_2_rate = test_algo(ucb_2,runs,step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average reward and best action rate of each step\n",
    "plt.figure(1,figsize=(15,6))\n",
    "\n",
    "# set the format of the y-axis as a percentage\n",
    "def to_percent(temp, position):\n",
    "    return '%1.0f'%(100*temp) + '%'\n",
    "\n",
    "x = range(step)\n",
    "plt.plot(x,ucb_2_reward,linewidth=0.5,color='r',label=\"ucb,c=2\")\n",
    "plt.plot(x,eps_01_reward,linewidth=0.5,label=\"epsilon_greedy,epsilon=0.1\")\n",
    "plt.plot(x,eps_001_reward,linewidth=0.5,color='g',label=\"epsilon_greedy,epsilon=0.01\")\n",
    "plt.plot(x,eps_00_reward,linewidth=0.5,color='orange',label=\"epsilon_greedy,epsilon=0\")\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('average reward')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.figure(2,figsize=(15,6))\n",
    "plt.plot(x,ucb_2_rate,linewidth=0.5,color='r',label=\"ucb,c=2\")\n",
    "plt.plot(x,eps_01_rate,linewidth=0.5,label=\"epsilon_greedy,epsilon=0.1\")\n",
    "plt.plot(x,eps_001_rate,linewidth=0.5,color='g',label=\"epsilon_greedy,epsilon=0.01\")\n",
    "plt.plot(x,eps_00_rate,linewidth=0.5,color='orange',label=\"epsilon_greedy,epsilon=0\")\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('%Optimal action')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.figure(3,figsize=(15,6))\n",
    "plt.plot(x,ucb_1_reward,linewidth=0.5,label=\"ucb,c=1\")\n",
    "plt.plot(x,ucb_2_reward,linewidth=0.5,color='r',label=\"ucb,c=2\")\n",
    "plt.plot(x,ucb_05_reward,linewidth=0.5,color='g',label=\"ucb,c=0.5\")\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('average reward')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.figure(4,figsize=(15,6))\n",
    "plt.plot(x,ucb_1_rate,linewidth=0.5,label=\"ucb,c=1\")\n",
    "plt.plot(x,ucb_2_rate,linewidth=0.5,color='r',label=\"ucb,c=2\")\n",
    "plt.plot(x,ucb_05_rate,linewidth=0.5,color='g',label=\"ucb,c=0.5\")\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('%Optimal action')\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BASE_ENV",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
